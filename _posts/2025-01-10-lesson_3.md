---
layout: post
title: "Lesson 3: Hyperparameters"
description: "Everyone knows the ingredients for a cake, but few can measure them masterfully."
media_subpath: /img/lesson_3
image:
  path: /lesson_3.jpg
date: 2025-01-10
math: true
categories: [Machine Learning]
tags: [machine-learning, introduction, learning-algorithms, data-science, ai-basics, model-training, ml-theory]     # TAG names should always be lowercase
---

{% include disclaimer.html %}

In the **[previous lesson](../lesson_2)**, we identified the fundamental ingredients to build a learning machine: a prediction function, a loss function, and a dataset. These components allow us to mathematically formalize our first learning machine.

However, defining the functions is not enough. We need to find the optimal parameters that enable our machine to make accurate predictions: the so-called **hyperparameters**.

<blockquote class="prompt-tip">
{% raw %}
The ingredients for our first learning machine:<br><br>
1. <strong>Prediction Function</strong>: <div id="eq-polynomial"> 
$$
\hat{y} = f(x) = \sum_{i=0}^p c_i x^i \tag{1}
$$
</div>
2. <strong>Loss Function</strong>: <div id="eq-loss">
$$
l(f(x), y) = (y - f(x))^2 \tag{2}
$$
</div>
3. <strong>Dataset</strong>: $$ D_n = \{ (x_1, y_1), \dots, (x_n, y_n) \} $$

{% endraw %}
</blockquote>

<blockquote class="prompt-warning">
{% raw %}
\( f \) and \( l \) have been chosen.<br> \( p \) and \( c_i \) need to be found.
{% endraw %}
</blockquote>

The variables we need to find are called **hyperparameters**.

$$p$$ and $$c_i$$ are hyperparameters.

Let’s get to know them.

<h3 id="p"><span class="me-2">\(p\) </span><a href="#p" class="anchor text-muted"></a></h3>

Imagine the following situation:
![Light mode only](/light_no_line.png){: .light }
![Dark mode only](/dark_no_line.png){: .dark }

What is the model that best fits this set of points?

A circle?

A parabola?

Or something else?

Before continuing, try to answer!

![Light mode only](/linear_regression_plot_light_mode.png){: .light }
![Dark mode only](/linear_regression_plot_dark.png){: .dark }

<blockquote class="prompt-tip"> If you answered <strong>line</strong>, congratulations: you're not a robot! </blockquote>

For us humans, it is evident that the model that best fits this set of points is a line.

To determine $$p$$, which represents the degree of the function (i.e., its complexity), it took us only a moment—a quick glance.

This is because intelligence is a uniquely human trait. However, we are extremely slow at performing calculations. I challenge you to find $$m$$, the slope of the line you immediately identified as the model. We would need to compute a line for every pair of points, minimize the error for each, and then select the line with the smallest overall error. In short, we’d go crazy!

For computers, it’s exactly the opposite: they are dumb and lack intuition, but they are incredibly fast. Finding parameters like $$m$$ and $$c_i$$ is robot’s play for them, whereas other parameters, like $$p$$, are much more complex to determine.

This is the core idea behind machine learning: transform everything that requires intelligence into a problem that can be solved with brute force and significant computational power.

<blockquote class="prompt-info">{% raw %} For a computer, some parameters are difficult to find, such as \( p \), while others are easy, like \( c_i \). {% endraw %}</blockquote>

Let’s see why finding $$c_i$$ is easy for a computer.

<h3 id="c"><span class="me-2">\(c_i\) </span><a href="#c" class="anchor text-muted"></a></h3>

Let’s revisit equation [$$ (1) $$](#eq-polynomial), our prediction function:

$$
\hat{y} = f(x) = \sum_{i=0}^p c_i x^i
$$

and assume that $$ p $$ is provided by an oracle. We want to find the optimal value of $$ c_i $$.

In other words, we aim to identify the best $$f(x)$$ from the set of possible $$f(x)$$, since changing $$c_i$$ alters the function $$f(x)$$.

How do we do this?

Our goal is to determine the value of $$c_i$$ that minimizes the average error over the available data (i.e., we want the predictions of the chosen function to be as close as possible to the actual values).

This can be expressed as:

$$
\min_{\underline{c}} \frac{1}{n} \sum_{i=1}^{n} \left( y_i - f(x_i) \right)^2 = \frac{1}{n} \sum_{i=1}^{n} \left( y_i - \sum_{j=0}^{p} c_j x_i^j \right)^2
$$

What I did here is take [$$ (2) $$](#eq-loss), the loss function, substitute the expression for $$ f(x) $$ with the prediction function, and minimize it with respect to $$ c_i $$, which is the vector containing all $$(p + 1)$$ parameters.

During this process, we can exclude the factor $$\frac{1}{n}$$ from the mean error. In fact, this multiplicative constant does not affect the result of the minimization. Thus:

$$
\min_{\underline{c}} \sum_{i=1}^{n} \left( y_i - f(x_i) \right)^2 = \sum_{i=1}^{n} \left( y_i - \sum_{j=0}^{p} c_j x_i^j \right)^2 =
$$

<div id="eq-matrix">
$$
=\|X \underline{c} - \underline{y}\|^2 \tag{3}
$$
</div>

To convince ourselves of this result, let’s define:

$$
\underline{c} = 
\begin{bmatrix}
c_0 \\
\vdots \\
c_p
\end{bmatrix} 
\in \mathbb{R}^{p+1}
$$

$$
\underline{y} = 
\begin{bmatrix}
y_1 \\
\vdots \\
y_n
\end{bmatrix} 
\in \mathbb{R}^n
$$

$$
X = 
\begin{bmatrix}
x_1^0 & \cdots & x_1^p \\
\vdots & \ddots & \vdots \\
x_n^0 & \cdots & x_n^p
\end{bmatrix} 
\in \mathbb{R}^{n \times (p+1)}
$$

and explicitly calculate:

$$
\|X \underline{c} - \underline{y}\|^2 = 
\left\|
\begin{bmatrix}
x_1^0 & x_1^1 & \cdots & x_1^p \\
\vdots & \vdots & \ddots & \vdots \\
x_i^0 & x_i^1 & \cdots & x_i^p \\
\vdots & \vdots & \ddots & \vdots \\
x_n^0 & x_n^1 & \cdots & x_n^p
\end{bmatrix}
\begin{bmatrix}
c_0 \\
c_1 \\
\vdots \\
c_p
\end{bmatrix}
-
\begin{bmatrix}
y_1 \\
\vdots \\
y_i \\
\vdots \\
y_n
\end{bmatrix}
\right\|^2
$$

Now, let’s focus on the $$i$$-th element of the norm. After multiplying the $$i$$-th row of $$X$$ with $$\underline{c}$$ and subtracting $$y_i$$, we get:

$$
\left( 
\begin{bmatrix}
x_i^0 & x_i^1 & \cdots & x_i^p
\end{bmatrix}
\begin{bmatrix}
c_0 \\
c_1 \\
\vdots \\
c_p
\end{bmatrix}
- y_i 
\right)^2
= 
\left( \sum_{j=0}^{p} c_j x_i^j - y_i \right)^2
$$

Thus, the $$i$$-th term within the squared norm is:

$$
\|X \underline{c} - \underline{y}\|^2 = \sum_{i=1}^{n} \left( y_i - \sum_{j=0}^{p} c_j x_i^j \right)^2
$$

which is exactly [$$ (3) $$](#eq-matrix). Voilà!

So, we need to minimize this quantity:

$$
\min_{\underline{c}} \|X \underline{c} - \underline{y}\|^2
$$

To address this problem, we first need to understand the mathematical structure of the function to minimize.

The function $$ \|X \underline{c} - \underline{y}\|^2 $$ is the square of a norm, which can be seen as a generalization of the quadratic form $$ ax^2 + bx + c $$ but in higher dimensions.

Graphically, this represents a generalized parabola in multidimensional space, known as a **paraboloid**.

![Light mode only](/paraboloid_light.png){: .light }
![Dark mode only](/paraboloid_dark.png){: .dark }
_A paraboloid. Fascinating, isn’t it?_

This structure is appealing because, in addition to being fascinating, it has a very useful property: there is a unique global minimum. In one dimension, to find the minimum of a quadratic function, we set the first derivative to zero.

Similarly, in higher dimensions, we use the **gradient**: we set the gradient of the function to zero to find the point where the minimum is achieved. Let’s do it together:

$$
\nabla_{\underline{c}} \left( \underline{c}'X'X\underline{c} - 2\underline{c}'X'\underline{y} + \underline{y}'\underline{y} \right) = \underline{0}
$$

$$
\cancel{2}X'X\underline{c} - \cancel{2}X'\underline{y} = \underline{0}
$$

$$
X'X\underline{c} = X'\underline{y}
$$

Now, to obtain $$c_i$$, we have two options:

1. We can treat &nbsp; $$ X'X\underline{c} = X'\underline{y} $$ &nbsp; as a linear system in the form $$ A \underline{x} = \underline{b} $$.  
2. Alternatively, we can solve:

$$
\underline{c} = (X'X)^+ X'\underline{y}
$$

where $$(X'X)^+$$ is the generalized inverse of the matrix, known as the **pseudoinverse**.

<blockquote class="prompt-info">
  <p><strong>The pseudoinverse</strong> is a mathematical tool used to solve linear systems when we cannot compute the regular inverse of a matrix.</p>

  <p>When does this happen?</p>

  <ol>
    <li>
      <strong>Non-square matrix.</strong>
      <ul>
        <li>This occurs, for example, when there are more variables than equations, meaning the data is limited relative to the model’s complexity.</li>
      </ul>
      <p>In such cases, the system has infinite solutions, and the pseudoinverse selects the simplest solution, i.e., the one with the smallest values.</p>
    </li>
    <li>
      <strong>Square but singular matrix.</strong>
      <ul>
        <li>This occurs, for example, when there are more equations than variables, meaning there is more data or observations than parameters to estimate.</li>
      </ul>
      <p>In this case, there is no solution that satisfies all equations, and the pseudoinverse finds the best possible solution, i.e., the one that minimizes the error between predicted and actual values.</p>
    </li>
  </ol>
</blockquote>

#### Case 1: Solving a single linear system
In the first case, we are considering the system:

$$ X'X\underline{c} = X'\underline{y} $$

and representing it as:

$$ A \underline{x} = \underline{b} $$

where:

- $$ X'X $$ represents the coefficient matrix $$ A $$.
- $$ X'y $$ is the vector of known terms $$ \underline{b} $$.
- $$ \underline{c} $$ is the vector of unknowns $$ \underline{x} $$.

Let’s calculate its complexity using the simplest linear system possible, for example:

$$
\begin{aligned}
3x + 4y &= 7 \\
7x + 3y &= 4
\end{aligned}
$$

In matrix form:

$$
\begin{bmatrix}
3 & 4 \\
7 & 3
\end{bmatrix}
\cdot
\begin{bmatrix}
x \\
y
\end{bmatrix}
=
\begin{bmatrix}
7 \\
4
\end{bmatrix}
$$

Where:

$$
A =
\begin{bmatrix}
3 & 4 \\
7 & 3
\end{bmatrix}
\quad
x =
\begin{bmatrix}
x \\
y
\end{bmatrix}
\quad
b =
\begin{bmatrix}
7 \\
4
\end{bmatrix}
$$

To solve the system, we can proceed using Gaussian elimination.

Subtract $$\frac{7}{3}$$ times the first row from the second row, and the new second row becomes:

$$
0 + \left(-\frac{7}{3} \cdot 4 + 3\right)y = 4 - \frac{7}{3} \cdot 7
$$

In matrix form, we then obtain:

$$
\begin{bmatrix}
3 & 4 \\
0 & -\frac{1}{3}
\end{bmatrix}
$$

In general, if we imagine $$ A $$ as a larger matrix, what we’ve just achieved would look something like this:

$$
A =
\begin{bmatrix}
. &   &   &   &   &   \\
0 & . &   &   &   &   \\
. & . & . &   &   &   \\
. & . & . & . &   &   \\
. & . & . & . & . &   \\
. & . & . & . & . & .
\end{bmatrix}
$$

And we can eliminate all numbers below the zero we just obtained:

$$
A =
\begin{bmatrix}
. &   &   &   &   &   \\
0 & . &   &   &   &   \\
0 & . & . &   &   &   \\
0 & . & . & . &   &   \\
0 & . & . & . & . &   \\
0 & . & . & . & . & .
\end{bmatrix}
$$

Then proceed with the second column below the main diagonal:

$$
A =
\begin{bmatrix}
. &   &   &   &   &   \\
0 & . &   &   &   &   \\
0 & 0 & . &   &   &   \\
0 & 0 & . & . &   &   \\
0 & 0 & . & . & . &   \\
0 & 0 & . & . & . & .
\end{bmatrix}
$$

Until we obtain this:

$$
A =
\begin{bmatrix}
. &   &   &   &   &   \\
0 & . &   &   &   &   \\
0 & 0 & . &   &   &   \\
0 & 0 & 0 & . &   &   \\
0 & 0 & 0 & 0 & . &   \\
0 & 0 & 0 & 0 & 0 & .
\end{bmatrix}
$$

From the last row, we can derive a first-degree equation in the form:

$$
x = \frac{b}{a}
$$

We can substitute this solution into the second-to-last row, obtaining another first-degree equation in the same form, and so on up to the top.

This way, we find the solution to the system.

For an $$n \times n$$ matrix, the computational complexity is 
$$
O(n^2)
$$ 
because we need to zero out half of the matrix, which has 
$$
n \times n = n^2
$$ 
elements. Dividing by $$2$$, we get 
$$
\frac{n^2}{2}
$$ 
and thus 
$$
O(n^2).
$$

#### Case 2: Solving using the pseudo-inverse

Now, let’s consider the more complex case where we solve the system using the pseudo-inverse:

$$
\underline{c} = (X'X)^+ X'\underline{y}
$$

Here, we are looking for an optimal solution valid for **all possible vectors** $$ \underline{y} $$.
In other words, we are analyzing the entire solution space of the matrix $$A = X'X$$, finding the best solution for any possible $$b$$.

For this reason, calculating the pseudo-inverse has a computational complexity of $$O(n^3)$$, higher than the first method, because we are processing more information. Naturally, if we perform something more complex, complete, and informative, we must pay a higher cost in terms of memory and computational power.

---

To summarize:
<blockquote class="prompt-tip">
<strong>Case 1: Solving with a direct method</strong>  
<div>
$$
X'X\underline{c} = X'\underline{y}
$$
</div>
representable as:  
<div>
$$
A \underline{x} = \underline{b}
$$
</div>
<ul>
  <li><strong>Type of solution</strong>: Finds a specific solution for a single linear system.</li>
  <li><strong>Computational complexity</strong>: 
  $$
  O(n^2)
  $$  
  because it zeros out half of the elements below the main diagonal.</li>
</ul>

<strong>Case 2: Solving with the pseudo-inverse</strong>  
<div>
$$
\underline{c} = (X'X)^+ X'\underline{y}
$$
</div>
<ul>
  <li><strong>Type of solution</strong>: Finds an optimal solution valid for all possible vectors \( \underline{y} \), analyzing the entire solution space.</li>
  <li><strong>Computational complexity</strong>: 
  $$
  O(n^3)
  $$  
  due to the calculation of the pseudo-inverse, which is more expensive in terms of memory and computational power.</li>
</ul>
</blockquote>

So, we have successfully found $$c_i$$. With the data, the predictive function, and the loss function, given $$p$$, we can write code to find the best coefficients!

Isn’t it amazing? For a robot, it certainly is!

With this lesson, we’ve had a glimpse of what hyperparameters are and how they play a crucial role in the complex task of learning. However, knowing how to find the parameters is just the beginning.

In the next lesson, we’ll dive into the chaos of reality: discovering why data doesn’t always follow our rules and how the complexity of the world influences the way we design models. We are only scratching the surface of a hidden world waiting to be uncovered.

See you next time!
