---
layout: post
title: "Lesson 4: Children of Chaos"
description: "Why classical statistics won't save us from a butterfly's wingbeat"
media_subpath: /img/lesson_4
image:
  path: /lesson_4.jpg
date: 2025-02-07
math: true
categories: [Machine Learning]
tags: [machine-learning, introduction, learning-algorithms, data-science, ai-basics, model-training, ml-theory]     # TAG names should always be lowercase
---

{% include disclaimer.html %}

In the **[previous lesson](../lesson_3)**, we tried to play with some key ingredients of machine learning: hyperparameters. We understood what differentiates us from machines and attempted to reduce the complexity of the world into solvable equations with a massive dose of computational power.

But are data really that predictable? What happens when apparent order gives way to the fragmented nature of reality? In this lesson, we will try to pierce this veil.

Let’s dive right in.

Suppose my data is generated by this oracle function:

![Light mode only](/function_light.png){: .light }
![Dark mode only](/function_dark.png){: .dark }

By defining this function, I am removing the machine learning hypothesis that I don’t know how the system works. In this case, I know how it works because I know the function that generates the data.

Let’s imagine that the oracle generates data subject to some noise:

![Light mode only](/points_light.png){: .light }
![Dark mode only](/points_dark.png){: .dark }

Now, from this data, I want to use my predictive function $$\hat{y} = f(x) = \sum_{i=0}^p c_i x^i $$ to find a function that best approximates the data.

The question we ask now is: what is the best $$p$$?

We will see that the best solution could be $$0$$, $$1$$, or $$2$$.

But what does it depend on?

It depends on:

1. The number $$n$$ of samples available.
2. The noise.

Let’s do an example to understand better.

Suppose that

$$p = n - 1$$

i.e., the degree of the polynomial equals the number of samples.

From the **[previous lesson](../lesson_3/#c)**, you should recall that calculating:

$$
\min_{\underline{c}} \|X \underline{c} - \underline{y}\|^2
$$

is equivalent to finding the coefficients $$c_i$$ that minimize the distance between the real data and my predictions.

If the polynomial degree equals the number of data points, the minimum we expect to see is:

$$
\min_{\underline{c}} \|X \underline{c} - \underline{y}\|^2 = 0,  \forall  D_n
$$

because when we choose a polynomial of degree $$p = n - 1$$, we are setting up the problem such that the polynomial passes exactly through all data points[^1].

In the case where the number of points is $$n = 3$$, the situation is as follows:

![Light mode only](/second_function_light.png){: .light }
![Dark mode only](/second_function_dark.png){: .dark }
_Make way! I pass through the data, so I'm perfect! Oh, no..._

The function deviates radically from the oracle function we hypothetically know to be correct, i.e., $$y=x^2$$.

And this is not at all what we would want, right?

Paradoxically (but actually, as we’ll see, not so paradoxically), we would have had a more effective result by choosing a polynomial of degree 1, i.e., a straight line:

![Light mode only](/third_function_light.png){: .light }
![Dark mode only](/third_function_dark.png){: .dark }
_The line approximates the oracle function much better than the curve passing through the data_

This means that minimizing over the data is not too clever, because instead of fitting the data, we are fitting the noise in the data.

To understand why this happens, we must enter the realm of statistics.

## Statistics

Let’s forget the notations introduced so far and redefine new ones.

From a variable $$X$$, we sample a series of observations.

$$ X \rightarrow  x_1, \ldots, x_n$$

- $$X$$ represents an uncertain phenomenon we are studying.
- The sample $$x_1, \ldots, x_n$$ is a set of $$n$$ values we observe in practice, independent and identically distributed (**i.i.d.**).

<blockquote class="prompt-tip">

<strong>i.i.d. Observations</strong> means:<br><br>

1. <strong>Independent</strong>: The value of one observation does not influence others.<br><br>

2. <strong>Identically Distributed</strong>: All observations come from the same probability distribution.<br><br>

</blockquote>

For example, imagine we want to estimate how many times a 3-year-old child asks "Why?" in an hour.

- $$X$$ is the random variable representing the number of "Why?" asked by a child in an hour.
- $$x_1, x_2, \ldots, x_n$$ each represent the number of "Why?" asked by each generic child in an hour. For example:
  $$
  x_1 = 20, \; x_2 = 30, \; x_3 = 15, \; x_4 = 25, \; x_5 = 35
  $$

In this context, i.i.d. means:
- We assume the data is **independent**, i.e., the number of "Why?" asked by one child does not influence the behavior of others.
- We assume the data is **identically distributed**, i.e., each child comes from the same *population* (e.g., curious 3-year-olds).

These assumptions are fundamental to applying the concepts we’ll explore.

### Expected Value

From statistics, we know that $$\mu$$, the **theoretical mean**, or expected value, is:

$$
\mu = \mathbb{E}_X[X] = \int_D x \cdot p(x) \, dx
$$

This tells us that the theoretical mean $$\mu$$ is calculated by weighting every possible value of $$X$$ (i.e., $$x$$) by its probability $$p(x)$$. $$D$$ is the domain of the probability distribution.
In our case, $$\mu$$ represents the average number of "Why?" asked by a child **in the entire population**, not just the sample. Hence, the integral.

However, the only way to estimate $$\mu$$ is through the **empirical mean**, because we cannot estimate all possible values, only those we know.

### Empirical Mean

The **empirical mean**, or sample mean, is calculated as:

$$
\bar{X} = \frac{1}{n} \sum_{i=1}^n x_i
$$

For example:
With $$n = 5$$ children:

$$
\bar{X} = \frac{1}{5} (x_1 + x_2 + x_3 + x_4 + x_5) = \frac{1}{5} (20 + 30 + 15 + 25 + 35) = \frac{125}{5} = 25
$$

Thus, we estimate that a child asks "Why?" 25 times on average in an hour.

### Estimator

The empirical mean $$\bar{X}$$ is an unbiased estimator of the theoretical mean $$\mu$$:

$$
\mathbb{E}_{\bar{X}}[\bar{X}] = \mu
$$

What does this mean?

It means that if we sample $$n$$ samples, calculate the mean, repeat this procedure an infinite number of times, and finally calculate the mean of all these means, we obtain the true value of the theoretical mean.

Let’s prove this.

1. The expectation of the sample mean is:

   $$
   \mathbb{E}_{\bar{X}}[\bar{X}] = \mathbb{E}_{x_1, \ldots, x_n} \left[\bar{X}\right] = \mathbb{E}_{x_1, \ldots, x_n} \left[\frac{1}{n} \sum_{i=1}^n x_i\right] 
   $$

2. Since the data is i.i.d., we can swap the summation with the expectation:

   $$
   \mathbb{E}_{x_1, \ldots, x_n} \left[\frac{1}{n} \sum_{i=1}^n x_i\right]  = \frac{1}{n} \sum_{i=1}^n \mathbb{E}_{x_i}[x_i]
   $$

3. And since the $$x_i$$ are identically distributed, we can say:

   $$
   \frac{1}{n} \sum_{i=1}^n \mathbb{E}_{x_i}[x_i]= \frac{1}{n} \cdot n \cdot \mu 
   $$

4. Therefore:

   $$
   \mathbb{E}_{\bar{X}}[\bar{X}] = \mu
   $$

This proves that the expected value of the empirical mean $$\bar{X}$$ coincides with the theoretical mean $$\mu$$.

---

But this is not enough.

<blockquote class="prompt-info">
In statistics, to obtain a complete and reliable estimate, we need to consider <strong>three fundamental quantities</strong>:<br><br>

&nbsp;&nbsp;&nbsp;&nbsp;1. <strong>Estimator</strong>: which we identified as the empirical mean thanks to its being an unbiased estimator of \( \mu \), the theoretical mean or expected value.<br><br>

&nbsp;&nbsp;&nbsp;&nbsp;2. <strong>Accuracy</strong>: represents how close a single sample estimate \(\bar{X}\) is to the theoretical mean \( \mu \). Accuracy depends on the <strong>variance of the estimator</strong>,
which must be sufficiently small to ensure that estimates are concentrated around \( \mu \).<br><br>

&nbsp;&nbsp;&nbsp;&nbsp;3. <strong>Confidence</strong>: concerns the probability that a sample estimate \(\bar{X}\) falls within a specific interval around \( \mu \). This concept underpins confidence intervals, which allow us to quantify the reliability of estimates.<br>

</blockquote>

### Accuracy

We have shown that the empirical mean $$\bar{X}$$ is an unbiased estimator of the theoretical mean $$\mu$$.
This means that, on average, the expected value of the estimator $$\bar{X}$$ coincides with $$\mu$$, i.e., it does not systematically tend to overestimate or underestimate the true parameter.
However, the property of unbiasedness is not sufficient to guarantee that each single sample estimate $$\bar{X}$$ is close to $$\mu$$.

This closeness is what we call **accuracy**.

To understand how accurate an estimator is, we must consider its **variance**.

#### Variance

The variance of the estimator determines how large the fluctuations of individual sample estimates can be. It quantifies, in other words, how dispersed the data is relative to the mean.
If the samples $$({x_1, \ldots, x_n})$$ are very close to the mean value $$\mu$$, the variance will be small.
If instead the samples are far from the mean, the variance will be large.

In other words, the larger the variance, the more scattered the data; the smaller it is, the more concentrated the data is around the mean.

Returning to our example of curious (or stressful, if you prefer) children, suppose we observe two distinct groups. In the first, each child asks "Why?" about 25 times, with small variations. In the second, some children ask only 5 "Why?", while others ask 50. The first group will have lower variance because the data is more concentrated around the mean, while the second group will have higher variance, reflecting greater dispersion.

The variance of a random variable $$X$$ is defined as:

$$
\sigma_X^2 = \mathbb{E}_X \left[ (X - \mu)^2 \right] =
$$

$$
=\mathbb{E}_X \left[ X^2 \right] - 2 \mathbb{E}_X \left[ X \mu \right] + \mathbb{E}_X \left[ \mu^2 \right]
$$

But due to the linearity of the expectation operator, $$\mu$$ can be factored out:

$$
2 \mathbb{E}_X \left[ X \mu \right] = 2 \mu \mathbb{E}_X \left[ X\right] = 2 \mu\mu = 2 \mu^2
$$

Similarly:

$$
\mathbb{E}_X \left[ \mu^2 \right] = \mu^2
$$

Thus, the variance becomes:

$$
\sigma_X^2 = \mathbb{E}_X \left[ (X - \mu)^2 \right] = \mathbb{E}_X \left[ X^2 \right] - 2 \mu^2 + \mu^2 = \mathbb{E}_X \left[ X^2 \right] - \mu^2
$$

The variance helps us prove the **Law of Large Numbers**. This law states that as the number of data points increases, the estimator $$\bar{X}$$ will get closer and closer to the theoretical mean $$\mu$$.

To prove this, let’s calculate the **variance of $$\bar{X}$$**.

$$
\sigma_{\bar{X}}^2 = \mathbb{E}_{\bar{X}} \left[\bar{X}^2\right] - \left(\mathbb{E}_{\bar{X}} \left[\bar{X}\right]\right)^2 = \mathbb{E}_{x_1, \ldots, x_n} \left\{ \frac{1}{n^2} \sum_{i=1}^n \sum_{j=1}^n x_i \cdot x_j \right\} - \mu^2 = \textcolor{#007FFF}{\bullet}
$$

Let’s focus on the double summation. We can observe that:

$$i = j \rightarrow x_i^2 $$

$$i \neq j \rightarrow x_i \cdot x_j $$

Since it’s a double summation, it can be represented as a matrix:

$$
\begin{bmatrix}
x_1^2 & \cdots & x_i x_j & \cdots & x_1 x_n \\
\vdots & \ddots & \vdots & & \vdots \\
x_i x_j & \cdots & (x_i)^2 & \cdots & x_i x_j \\
\vdots & & \vdots & \ddots & \vdots \\
x_n x_1 & \cdots & x_i x_j & \cdots & x_n^2
\end{bmatrix}
$$

where:

1. Along the main diagonal, we have elements $$x_i^2 $$, which are $$n$$ in total (equal to the number of terms in the summation).

2. Everywhere except the main diagonal, we have elements $$x_i \cdot x_j$$, which amount to $$n^2 - n$$ elements (total matrix elements $$n^2$$ minus the $$n$$ diagonal elements).

Now, let’s separate the diagonal terms from the off-diagonal terms:

$$\textcolor{#007FFF}{\bullet} = \frac{1}{n^2} \sum_{i=1}^n \sum_{\substack{j=1 \\ j \neq i}}^n \underbrace{\mathbb{E}_{x_i, x_j}(x_i \cdot x_j)}_{\mu^2} + \frac{1}{n^2} \sum_{i=1}^n \underbrace{\mathbb{E}_{x_i}[x_i^2]}_{\mathbb{E}_X[x^2]} - \mu^2 = \textcolor{\green}{\bullet}$$

We can assert that:

1. $$\mathbb{E}_{x_i, x_j}(x_i \cdot x_j) = \mu^2 $$ because $$x_i$$ and $$x_j$$ are independent, so:

   $$\mathbb{E}_{x_i, x_j}(x_i \cdot x_j) = \mathbb{E}[x_i] \cdot \mathbb{E}[x_j] = \mu \cdot \mu = \mu^2$$

2. $$\mathbb{E}_{x_i}[x_i^2] = \mathbb{E}_X[x^2]$$ because all $$x$$ are i.i.d..

Considering that for $$i \neq j $$ there are $$n^2-n$$ elements and for $$i = j $$ there are $$n$$ elements, we can write:

$$
\textcolor{\green}{\bullet} = \frac{n^2 - n}{n^2} \mu^2 + \frac{n}{n^2} \mathbb{E}_X[x^2] - \mu^2 =
$$

$$
= \frac{\cancel{n^2} - n \cancel{-n^2}}{n^\cancel{2}} \mu^2 + \frac{1}{n} \mathbb{E}_X[x^2] =
$$

$$
= \frac{1}{n} \left(\mathbb{E}_X[x^2] - \mu^2\right)
$$

And by definition:

$$
= \frac{\sigma_X^2}{n}.
$$

Thus, we have shown that:

$$
\sigma_{\bar{X}}^2 = \frac{\sigma_X^2}{n}.
$$

This result is a key piece for proving the **Law of Large Numbers**, as it clearly shows that increasing the number of samples causes the variance of our estimator to tend to zero.
In other words, our estimator $$\bar{X}$$ converges to $$\mu$$. The more data we collect, the more likely our measurement is to approach what we want to measure.

For example, if we observe only five children, our estimate of the average number of "Why?" might be influenced by random variations. For instance, they might all have been distracted by a squirrel outside the window. Or you might have encountered a group of super-organized children with a handwritten list of 50 "Why?" prepared in advance. But if we observe 100 or 1000 children, random fluctuations cancel out, and our estimate becomes increasingly precise, approaching the theoretical mean $$\mu$$.

However, this result only tells us that, with a sufficient number of samples, the estimator will be close to the theoretical mean. It does not tell us **how close** we are to what we want to measure.

We have an unbiased estimator and a measure of convergence ($$\bar{X} \to \mu$$), but we are still missing a fundamental ingredient: **confidence** in our estimate.

### Confidence

**Confidence** in our estimate refers to the probability that the sample estimator $$\bar{X}$$ falls within a specific interval around the theoretical mean $$\mu$$. This concept is tied to the construction of confidence intervals, which allow us to say **how much we can trust** our estimates.

To analyze this confidence, we focus on the probability that $$\bar{X}$$ deviates from $$\mu$$ by more than a threshold $$\epsilon$$:

$$
P\{ |\bar{X} - \mu| \geq \epsilon \}
$$

Using the definition of variance, we know:

$$
\sigma_{\bar{X}}^2 = \int_{-\infty}^{\infty} (\bar{X} - \mu)^2 \, p(\bar{X}) \, d\bar{X} \geq
$$

Since we are integrating a positive function, we can bound this quantity from below:

$$
\geq \int_{|\bar{X} - \mu| > \epsilon} (\bar{X} - \mu)^2 \, p(\bar{X}) \, d\bar{X}
$$

We integrate over $$\vert\bar{X} - \mu\vert > \epsilon$$ because we are focusing on events where the estimator significantly deviates from the theoretical mean.

Visually, it’s as if we are integrating our function over an interval like this:

![Light mode only](/integration_light.png){: .light }
![Dark mode only](/integration_dark.png){: .dark }

Replace $$ (\bar{X} - \mu)^2 $$ with the minimum possible value in that region, i.e., $$\epsilon^2$$. We get:

$$
\int_{|\bar{X} - \mu| > \epsilon} (\bar{X} - \mu)^2 \, p(\bar{X}) \, d\bar{X} \geq
$$

Since $$\vert\bar{X} - \mu\vert > \epsilon$$, this quantity is bounded below by:

$$ 
\geq 
\int_{|\bar{X} - \mu| > \epsilon} \epsilon^2 p(\bar{X}) \, d\bar{X} =
$$

$$ 
=\epsilon^2 \int_{|\bar{X} - \mu| > \epsilon} p(\bar{X}) \, d\bar{X}
$$

This integral is precisely the probability that $$\vert\bar{X} - \mu\vert \geq \epsilon$$. By definition of probability density, we are integrating the density of $$\bar{X}$$ only when $$\vert\bar{X} - \mu\vert \geq \epsilon$$. Thus, we can rewrite:

$$
\sigma_{\bar{X}}^2 \geq \epsilon^2 P\{ |\bar{X} - \mu| \geq \epsilon \}
$$

From this inequality, we isolate the probability to obtain an important result known as **Chebyshev’s inequality**:

$$
P\{ |\bar{X} - \mu| \geq \epsilon \} \leq \frac{\sigma_{\bar{X}}^2}{\epsilon^2}
$$

<blockquote class="prompt-tip">

Since \( \sigma_{\bar{X}}^2 = \frac{\sigma_X^2}{n} \), we can further write:

$$
P\{ |\bar{X} - \mu| \geq \epsilon \} \leq \frac{\sigma_X^2}{n \epsilon^2}
$$

which is the <strong>Law of Large Numbers</strong>.
</blockquote>

Now assume $$X$$ is bounded, i.e., $$X \in [0, 1] $$. After all, everything we know (except the universe) is bounded. To be fair, we have yet to prove the universe is infinite.

We can write:

$$
P\{ |\bar{X} - \mu| \geq \epsilon \} \leq \frac{\sigma_X^2}{n \epsilon^2} \leq \frac{1}{n \epsilon^2}
$$

This is because the variance will always be constrained by the domain of the variable’s possible values.

Now we have everything we need:

<div id="eq-big-numbers"> 
$$
P\left\{ |\underbrace{\bar{X}}_{\text{Estimator}} - \mu| \geq \underbrace{\epsilon}_{\text{Accuracy}} \right\} \leq \underbrace{\frac{1}{n \epsilon^2}}_{\text{Confidence}} \tag{1}
$$
</div>

What I’m asserting is this:

![Light mode only](/confidence_light.png){: .light }
![Dark mode only](/confidence_dark.png){: .dark }

To estimate the theoretical mean $$\mu$$, I perform multiple measurements (represented by dots on the segments). Each measurement provides an estimate of the mean from a sample.

For each estimate, I calculate a confidence interval (horizontal segments). These intervals represent the uncertainty of the estimate: a range of values within which $$\mu$$ should lie with a certain probability (e.g., 99%).

Most intervals include $$\mu$$. Only a small percentage of intervals exclude $$\mu$$ (e.g., the green segment).  
This indicates the estimate is correct relative to the chosen confidence level.

A shorter segment indicates higher precision: the estimate is closer to $$\mu$$, and uncertainty is lower.

A longer confidence interval means we are less certain about $$\mu$$’s location: the estimate is less precise.

<blockquote class="prompt-tip">

Confidence and accuracy are connected: more confidence requires less accuracy; more accuracy requires less confidence.
</blockquote>

This relationship becomes clearer if we rewrite [$$ (1) $$](#eq-big-numbers):

$$
P\left\{ |\underbrace{\bar{X}}_{\text{Estimator}} - \mu| \geq \underbrace{\epsilon}_{\text{Accuracy}} \right\} \leq \frac{\sigma_X^2}{n \epsilon^2} \leq \underbrace{\frac{1}{n \epsilon^2}}_{\text{Confidence}} = \delta 
$$

and implicitly derive:

$$
\epsilon = \sqrt{\frac{1}{n \delta}}
$$

Or explicitly:

$$
|\bar{X} - \mu| \underset{1 < \delta}{\leq} \sqrt{\frac{1}{n \delta}}
$$

This means the sample mean $$\bar{X}$$ lies within a distance of $$ \sqrt{\frac{1}{n \delta}} $$ from $$\mu$$ with probability $$1 - \delta$$.

Alternatively:

<div id="eq-confidence-bound"> 
$$
\mu \underset{(1 - \delta)}{\leq} \bar{X} + \sqrt{\frac{1}{n \delta}} \quad \quad X \in [0, 1], \quad x_1, \dots, x_n \rightarrow i.i.d. \tag{2}
$$
</div>

If $$\mu$$ is the probability of death from a drug, and I administer the drug to 1000 people, I can state that this probability is less than or equal to the measured value from 1000 people plus $$\sqrt{\frac{1}{1000 \cdot \delta}}$$, where $$\delta$$ is the desired confidence level. 

For example, for 99% accuracy, set $$\delta = 0.01$$.

## The Flapping of a Butterfly’s Wings

But why did we do all this?

To show that everything we’ve done so far to build the learning machine is based on the formula [$$ (2) $$](#eq-confidence-bound).

Let’s revisit our **[ingredients](../lesson_2/)** and return to the domain of machine learning. We had:

**Dataset**: $$ D_n = \{ (x_1, y_1), \dots, (x_n, y_n) \}, \quad x \in \mathbb{R},\quad y \in \mathbb{R} $$

**Prediction function**: $$ \hat{y} = f(x) = \sum_{i=0}^p c_i x^i $$

**Loss function**: $$ l(f(x), y) = (y - f(x))^2 $$

And we wanted to minimize the error on the data with:

<div id="eq-min"> 
$$
\frac{1}{n} \sum_{i=1}^n (y_i - f(x_i))^2 \tag{3}
$$
</div>

which is an estimator of:

$$
\mathbb{E}_{x, y} \left( y - f(x) \right)^2
$$

But let’s focus and look at them through the statistical notations introduced in this lesson:

$$
\underbrace{ \frac{1}{n} \sum_{i=1}^n \underbrace{({y_i - f(x_i)})^2}_{x_i}}_{\bar{X}} 
\quad \longleftrightarrow \quad 
\underbrace{\mathbb{E}_{x, y} \underbrace{(y - f(x))^2}_{X}}_{\mu}
$$

<blockquote class="prompt-warning">
The notations under the braces refer to the statistical terms introduced in this lesson.<br>

The formulas without braces represent the notations from previous machine learning lessons.<br>

These are <strong>distinct domains</strong>!
</blockquote>

There seems to be a correspondence with statistics! But be careful.

For the [left](#eq-min) quantity, we computed the minimum. Why did we do that?

Because we assumed that if we could reduce errors on our data, the error relative to the population would also be smaller.

This reasoning is based on the idea that the empirical mean (calculated on the data) is a good estimator of the true mean (the one over the real data distribution).

However, we haven’t verified the underlying assumptions to assert this.

In fact, the statistical hypotheses imply that the errors (i.e., discrepancies between model predictions and observed values) are i.i.d. (independent and identically distributed).

This means the data must also be i.i.d., as non-i.i.d. data could never generate i.i.d. errors.

If this assumption isn’t met, we can’t guarantee that the empirical error calculated on collected data is a good estimator of the true error (the one the model would make on new data).

For example, without independence, we couldn’t prove the Law of Large Numbers. Independence ensures each data point brings new information and isn’t redundant with others. If data were dependent, no amount of measurements would help—we’d just be repeating the same information.

"Identically distributed" means the system isn’t changing over time. To predict the future, we need exactly this: the future to be similar to the past.  
And "similar" here means identically distributed.

But in reality, data is neither independent nor identically distributed.

In reality, data is born of chaos.

The flap of a butterfly’s wings in Brazil can cause a tornado in Texas[^2].

The displacement of a single electron by a billionth of a centimeter at a given moment could mean the life or death of a man a year later[^3].

## Conclusion

Now suppose we don’t learn from the data, i.e., we don’t minimize with respect to the model parameters $$\underline{c}$$.

In a context where data is i.i.d., we can say the empirical error (calculated on the data) is a good estimator of the true error (calculated on the entire population).

This is analogous to taking a system, applying a loss function, and measuring its performance: like measuring room temperature or testing a machine to see if it works. Here, we’re only using statistics because we’re simply measuring something.

However, the interesting case is when we **learn** from the data.

In this scenario, we don’t just measure—we try to **learn**: we want to find the function (or model) that works best on observed data.

This process changes the rules:

1. Try a function (i.e., a hypothesis or model).
2. Measure the empirical error on the data.
3. Modify the function based on results to minimize error.
4. Repeat.

The theory behind machine learning isn’t based on classical statistics but on **statistical learning theory**.

This distinction is critical. It means we must adjust our approach, i.e., [$$ (2) $$](#eq-confidence-bound), to account for the learning process.

When we minimize the [loss function](#eq-min) to fit a model to data, we break the independence assumption: errors are no longer independent.

This happens because the optimized model is built using all data simultaneously, so the error on one data point is influenced by errors on others.

This interdependence introduces greater complexity than the empirical error: we can no longer treat it directly as a simple estimator of the true error.

Minimizing empirical error might seem natural, but it hides a trap: overfitting to the data. This is where statistics and machine learning diverge. Classical statistics describes observed phenomena, while machine learning aims to learn from them to predict the unknown. This is exactly what we’ll explore in the next lesson.

See you next time!

---

### A Pinch of Irony

If a child exceeds **30 "Why?"s** in an hour, we might classify it as an "off-scale" case. At this point, we can start predicting adult responses using probability and statistics:

1. **"Because yes!"**: Short and efficient, ideal when adult processing time drops after the first 20 "Why?"s.

2. **"Why do you ask?"**: An inquisitorial strategy mirroring the child’s behavior to confuse them. Effective 50% of the time to induce a pause (but beware: may trigger an infinite "Why?" loop).

3. **"Because Santa Claus doesn’t exist!"**: The last resort, a desperate move when patience is exhausted. Sure, it halts the "Why?" stream, but be warned: an 88% risk of triggering tears, and the child might sue you.

According to our study, the average number of "Why?"s is $$25$$. Exceeding $$35$$ occurs in less than **16% of cases**. The highest recorded "Why?" count in an hour is 3600. In this case, the adult reportedly achieved a mystical state, abandoning all hope and replying, "Because the universe is infinite," before levitating into the sky at light speed, imploding 8 minutes and 20 seconds later into the star we now call the Sun.

---

[^1]: Since $$p = n - 1$$, the matrix $$X$$ is square (size $$n \times n$$) and invertible (assuming distinct $$x_i$$). This means the system $$ X'X\underline{c} = X'\underline{y} $$ has a unique solution: $$\underline{c} = (X'X)^{-1} X'\underline{y}$$, i.e., the coefficients $$c_i$$ match those of the interpolating polynomial passing exactly through all data points.

[^2]: Famous phrase by Edward Lorenz.

[^3]: Alan Turing, *Computing Machinery and Intelligence*, 1950.