---
layout: post
title: "Lesson 2: Ingredients to Build a Learning Machine"
description: "Exploring the essential components for a first prototype of a learning machine"
media_subpath: /img/lesson_2
image:
  path: /lesson_2.jpg
date: 2024-11-03
math: true
categories: [Machine Learning]
tags: [machine-learning, introduction, learning-algorithms, data-science, ai-basics, model-training, ml-theory]     # TAG names should always be lowercase
---

In the **[first lesson](../lesson_1)**, we explored the idea of building a *learning machine*, a system capable of learning from the past to predict the future. We discussed the role of observable data $$ x $$ and target variables $$ y $$, how they can be represented, and the computational challenges that can arise from this representation. Additionally, we saw how the correlation between these datas allows us to make estimates, while being aware of the noise and our incomplete knowledge of the original system.

<blockquote class="prompt-tip">
Remember?  
We learned that both the input \( \underline{x} \) and the target \( \underline{y} \) can be represented as vectors of elements belonging to \( \mathbb{R}^d \) and \( \mathbb{R}^p \), respectively, where \( d \) and \( p \) represent the dimensions of the vector, i.e., the number of components.
</blockquote>

In this **second lesson**, we will go further by analyzing the fundamental ingredients needed to build a *learning machine*.

Let's dive in.

<h2 id="predictor"><span class="me-2">Prediction Function</span><a href="#predictor" class="anchor text-muted"></a></h2>

Our goal is to build a learning model, i.e., a learning machine, which can be represented as:

$$
\underline{\hat{y}} = \underline{f}(\underline{x})
$$

Since $$ \underline{\hat{y}} $$ is a vector, we can think of predicting each element $$ \hat{y_i} $$ individually, using a specific function for each component:

$$
\hat{y_i} \in \mathbb{R} = f_i(\underline{x})
$$

Thus, the general problem we want to solve reduces to:

$$
\hat{y} \in \mathbb{R} = f(\underline{x})
$$

which means estimating a real value given a vector of possibilities.

But since we're building our first machine, why not simplify it further?

Instead of considering $$ \underline{x} $$ as a vector, let’s treat it as a simple scalar $$ x $$:

$$
\hat{y} = f(x)
$$

<blockquote class="prompt-info">
For our first learning machine, let’s consider:  
$$ x \in \mathbb{R}, \quad \hat{y} \in \mathbb{R} $$

We are thus assuming we are dealing with scalars only (where usually we deal with vectors or worse).
</blockquote>

Returning to our model, we now know what to put into our learning machine:

![Light mode only](/learningmachine.png){: .light }
![Dark mode only](/learningmachinedark.png){: .dark }
_Refining our model_

But ultimately, how do I define the function?

There are infinite ways to define it since there are infinite functions that, given an input, produce an output, so I have to choose.

Is there an optimal choice?

No!

Because according to the **No Free Lunch theorem**, under the conditions we initially imposed (i.e., we only know the input $$ x $$ and are unaware of both the system and the noise), any choice we make can be optimal in one case and terrible in another. To predict the future with reasonable computational complexity, we must compromise.

<blockquote class="prompt-danger">
<strong>No Free Lunch</strong>: Whatever we do won’t be optimal; instead, we settle for the choice that works best among those tried previously.
</blockquote>

So, we choose the simplest function we know, namely:

$$
\hat{y} = f(x) = \sum_{i=0}^p c_i x^i
$$

In fact, with a polynomial of sufficiently high degree, we can approximate any function we want.

<h2 id="loss-function"><span class="me-2">Loss Function</span><a href="#loss-function" class="anchor text-muted"></a></h2>

Now I need to define better what I mean by a good approximation $$ \hat{y} \approx y $$.

I define a **loss function**:

$$ l(\hat{y}, y) \rightarrow \mathbb{R} $$

The loss function takes as input the model's predictions and the actual real-world values (targets) and returns a real number representing the error (loss) associated with those predictions.

How do I choose it?

We can no longer rely on the mathematical approximation models we know (Taylor or Fourier) because we started with different premises.

We choose the **mean squared error**:

$$ l(f(x), y) = (y - f(x))^2 $$

We particularly like this loss function because it is infinitely differentiable and has no discontinuities.

<h2 id="dataset"><span class="me-2">Dataset</span><a href="#dataset" class="anchor text-muted"></a></h2>

Finally, here are our data (our real-world observations):

$$ D_n = \{ (x_1, y_1), \ldots, (x_n, y_n) \} $$

where $$ n $$ is the number of observations.

<h2 id="nutshell"><span class="me-2">In a Nutshell</span><a href="#nutshell" class="anchor text-muted"></a></h2>

<blockquote class="prompt-tip">
{% raw %}
The ingredients for our first learning machine:<br><br>
1. <strong>Prediction Function</strong>: <div id="eq-polynomial"> 
$$
\hat{y} = f(x) = \sum_{i=0}^p c_i x^i \tag{1}
$$
</div>
2. <strong>Loss Function</strong>: <div id="eq-loss">
$$
l(f(x), y) = (y - f(x))^2 \tag{2}
$$
</div>
3. <strong>Dataset</strong>: $$ D_n = \{ (x_1, y_1), \dots, (x_n, y_n) \} $$

{% endraw %}
</blockquote>

<blockquote class="prompt-warning">
{% raw %}
\( p \) and \( c_i \) are to be determined.<br> \( f \) and \( l \) are chosen by us.
{% endraw %}
</blockquote>

Now we have all the fundamental ingredients to build a first version of our learning machine: a prediction function, a loss function, and a dataset of observations.

However, knowing which functions to use and with which parameters is not enough: we also need to find the best values for these parameters.

In the next article, we will explore how to do this by analyzing different methods to solve linear systems and optimize our machine.

See you next time!
