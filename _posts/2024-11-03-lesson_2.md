---
layout: post
title: "Lesson 2: Ingredients to Build a Learning Machine"
description: "Exploring the essential components for a first prototype of a learning machine"
media_subpath: /img/lesson_2
image:
  path: /lesson_2.jpg
date: 2024-11-03
math: true
categories: [Machine Learning]
tags: [machine-learning, introduction, learning-algorithms, data-science, ai-basics, model-training, ml-theory]     # TAG names should always be lowercase
---

{% include disclaimer.html %}

In the **[first lesson](../lezione_1)**, we explored the idea of building a *learning machine*, a system capable of learning from the past to predict the future. We discussed the role of observable data $$ x $$ and target variables $$ y $$, how they can be represented, and the computational challenges that can arise from this representation. Additionally, we examined how the correlation between these data allows us to make estimates, while being aware of the presence of noise and our incomplete knowledge of the original system.

<blockquote class="prompt-tip">
Remember? <br>We saw that both the input \( \underline{x} \) and the target \( \underline{y} \) can be represented as vectors of elements belonging to \( \mathbb{R}^d \) and \( \mathbb{R}^p \), respectively, where \( d \) and \( p \) represent the dimensions of the vector, i.e., the number of components.
</blockquote>

In this **second lesson**, we’ll go further, analyzing the fundamental ingredients for building a *learning machine*.

Let’s get to the point.

<h2 id="predictor"><span class="me-2">Prediction Function</span><a href="#predictor" class="anchor text-muted"></a></h2>

Our goal is to build a learning model, a learning machine, which can be represented as follows:

$$
 \underline{\hat{y}} = \underline{f}(\underline{x})
$$

Since $$\underline{\hat{y}}$$ is a vector, we can think of predicting each element $$ \hat{y_i} $$ individually, using a specific function for each component:

$$
 \hat{y_i} \in \mathbb{R} = f_i(\underline{x})
$$

Therefore, the general problem we want to solve reduces to:

$$
 \hat{y} \in \mathbb{R} = f(\underline{x})
$$

i.e., estimating a real value given a vector of possibilities.

But since we are building our first machine, why not simplify even further?

Instead of taking $$\underline{x} $$ as a vector, let’s consider it as a simple scalar $$x$$:

$$
 \hat{y} = f({x})
$$

<blockquote class="prompt-info"> For our first learning machine, let’s consider: <br>
$$ x \in \mathbb{R}, \quad \hat{y} \in \mathbb{R} $$

This means we are assuming we are only dealing with scalars (where we usually have vectors or even worse stuff). </blockquote>

Returning to our model, we now know what to put in our learning machine:

![Light mode only](/learningmachine.png){: .light }
![Dark mode only](/learningmachinedark.png){: .dark }
_We are refining our model_

But ultimately, how do I define the function?

There are infinite ways to define it, as there are infinite functions that, given an input, produce an output, so I must choose.

Is there an optimal choice?

No!

According to the **No Free Lunch** theorem, given the initial conditions we’ve set (i.e., we only know the input $$ x $$ and nothing about the system or the noise), any choice we make can be optimal in one case and terrible in another. To predict the future with reasonable computational complexity, we must therefore make compromises.

<blockquote class="prompt-danger">
<strong>No Free Lunch</strong>: Whatever we do won’t be optimal; instead, we settle for the choice that performs best among those previously tested.
</blockquote>

So, we choose the simplest function we know:

$$
\hat{y} = f(x) = \sum_{i=0}^p c_i x^i 
$$

With a polynomial of sufficiently high degree, we can approximate any function we want.

<h2 id="loss-function"><span class="me-2">Loss Function</span><a href="#loss-function" class="anchor text-muted"></a></h2>

Now I need to define what I mean by a good approximation $$ \hat{y} \approx y $$.

I define a **loss function**:

$$ l(\hat{y}, y) \rightarrow \mathbb{R} $$

The loss function takes as input the model's predictions and the actual target values (ground truth) and returns a real number representing the error (the loss) associated with those predictions.

### How do I choose it?

I can no longer rely on the mathematical approximation models we know (Taylor or Fourier) because we started from different premises.

Let’s choose the **mean squared error**:

$$ l(f(x), y) = (y - f(x))^2 $$

This loss function is particularly appealing because it is infinitely differentiable and does not present any discontinuities.

<h2 id="dataset"><span class="me-2">Dataset</span><a href="#dataset" class="anchor text-muted"></a></h2>

Finally, here is our data (our observations of the real world):

$$ D_n = \{ (x_1, y_1), \ldots, (x_n, y_n) \} $$

where $$ n $$ is the number of observations.

<h2 id="nutshell"><span class="me-2">In a Nutshell</span><a href="#nutshell" class="anchor text-muted"></a></h2>

<blockquote class="prompt-tip">
{% raw %}
The ingredients for our first learning machine:<br><br>
1. <strong>Prediction Function</strong>: <div id="eq-polynomial"> 
$$
\hat{y} = f(x) = \sum_{i=0}^p c_i x^i \tag{1}
$$
</div>
2. <strong>Loss Function</strong>: <div id="eq-loss">
$$
l(f(x), y) = (y - f(x))^2 \tag{2}
$$
</div>
3. <strong>Dataset</strong>: $$ D_n = \{ (x_1, y_1), \dots, (x_n, y_n) \} $$

{% endraw %}
</blockquote>

<blockquote class="prompt-warning">
{% raw %}
\( p \) and \( c_i \) need to be found.<br> \( f \) and \( l \) have been chosen.
{% endraw %}
</blockquote>

Now we have all the fundamental ingredients to build a first version of our learning machine: a prediction function, a loss function, and a dataset of observations.

However, knowing which functions to use and with which parameters is not enough: we also need to find the best values for these parameters.

In the next article, we’ll explore how to do this by analyzing different methods for solving linear systems and optimizing our machine.

See you next time!
