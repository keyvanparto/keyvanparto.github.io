---
layout: post
title: "Lesson 3: Hyperparameters"
description: "Everyone knows the ingredients for a cake, but few can measure them masterfully."
media_subpath: /img/lesson_3
image:
  path: /lesson_3.jpg
date: 2024-11-20
math: true
categories: [Machine Learning]
tags: [machine-learning, introduction, learning-algorithms, data-science, ai-basics, model-training, ml-theory]     # TAG names should always be lowercase
---

In the **[last lesson](../lesson_2)**, we identified the fundamental ingredients to build a learning machine: a prediction function, a loss function, and a dataset. These components allow us to mathematically formalize our first learning machine.

However, defining the functions is not enough. We need to find the optimal parameters that allow our machine to make accurate predictions: the so-called **hyperparameters**.

<blockquote class="prompt-tip">
{% raw %}
The ingredients for our first learning machine:<br><br>
1. <strong>Prediction Function</strong>: <div id="eq-polynomial"> 
$$
\hat{y} = f(x) = \sum_{i=0}^p c_i x^i \tag{1}
$$
</div>
2. <strong>Loss Function</strong>: <div id="eq-loss">
$$
l(f(x), y) = (y - f(x))^2 \tag{2}
$$
</div>
3. <strong>Dataset</strong>: $$ D_n = \{ (x_1, y_1), \dots, (x_n, y_n) \} $$

{% endraw %}
</blockquote>

<blockquote class="prompt-warning">
{% raw %}
\( f \) and \( l \) are chosen by us.<br> \( p \) and \( c_i \) are to be determined.
{% endraw %}
</blockquote>

The variables we need to determine are called **hyperparameters**.

$$p$$ and $$c_i$$ are hyperparameters.

Let’s get to know them.

<h3 id="p"><span class="me-2">\(p\) </span><a href="#p" class="anchor text-muted"></a></h3>

Imagine the following situation:
![Light mode only](/light_no_line.png){: .light }
![Dark mode only](/dark_no_line.png){: .dark }

What model best approximates this series of points?

A circle?

A parabola?

Or something else?

Before proceeding, take a moment to answer!

![Light mode only](/linear_regression_plot_light_mode.png){: .light }
![Dark mode only](/linear_regression_plot_dark.png){: .dark }

<blockquote class="prompt-tip"> If you answered <strong>line</strong>, congratulations: you're not a robot! </blockquote>

For us humans, it’s clear that the model that best approximates this series of points is a line.

To determine $$p$$, which represents the degree of the function (and thus its complexity), it only took us a glance.

This is because intelligence is a distinctly human trait, but we are extremely slow at calculations. I challenge you to find $$m$$, the slope of the line you instantly guessed as the model. We would need to compute a line for every pair of points, minimize the error for each, and then choose the line with the smallest total error. In short, we’d go crazy!

For computers, it works the other way around: they are dumb and lack intuition, but they are incredibly fast. Finding parameters like $$m$$ and $$c_i$$ is robot’s play for them, whereas other parameters, like $$p$$, are much harder to determine.

The idea behind machine learning is exactly this: transform everything that requires intelligence into a problem solvable with brute force and a lot of computational power.

<blockquote class="prompt-info">{% raw %} For a computer, some parameters like \( p \) are hard to determine, while others like \( c_i \) are easy. {% endraw %}</blockquote>

Let’s see why it’s easy for a computer to find $$c_i$$.

<h3 id="c"><span class="me-2">\(c_i\) </span><a href="#c" class="anchor text-muted"></a></h3>

Let’s revisit equation [$$ (1) $$](#eq-polynomial), which defines our prediction function:

$$
\hat{y} = f(x) = \sum_{i=0}^p c_i x^i
$$

and assume that $$ p $$ is provided by an oracle. We want to find the optimal value of $$ c_i $$.

In other words, we want to identify the best $$f(x)$$ among the set of possible $$f(x)$$ functions, as changing $$c_i$$ alters the function $$f(x)$$.

How do we do this?

Our goal is to determine the value of $$c_i$$ that minimizes the mean error on the available data (i.e., we want the predictions of the chosen function to be as close as possible to the real values).

This can be expressed as:

$$
\min_{\underline{c}} \frac{1}{n} \sum_{i=1}^{n} \left( y_i - f(x_i) \right)^2 = \frac{1}{n} \sum_{i=1}^{n} \left( y_i - \sum_{j=0}^{p} c_j x_i^j \right)^2
$$

What I did here is take [$$ (2) $$](#eq-loss), the loss function, substitute $$ f(x) $$ with the prediction function, and minimize it with respect to $$ c_i $$, which is the vector containing all $$(p + 1)$$ parameters.

During this process, we can exclude the factor $$\frac{1}{n}$$ from the mean error, as this constant multiplier does not affect the minimization result. Therefore:

$$
\min_{\underline{c}} \sum_{i=1}^{n} \left( y_i - f(x_i) \right)^2 = \sum_{i=1}^{n} \left( y_i - \sum_{j=0}^{p} c_j x_i^j \right)^2 =
$$

<div id="eq-matrix">
$$
=\|X \underline{c} - \underline{y}\|^2 \tag{3}
$$
</div>

To verify this result, we define:

$$
\underline{c} = 
\begin{bmatrix}
c_0 \\
\vdots \\
c_p
\end{bmatrix} 
\in \mathbb{R}^{p+1}
$$

$$
\underline{y} = 
\begin{bmatrix}
y_1 \\
\vdots \\
y_n
\end{bmatrix} 
\in \mathbb{R}^n
$$

$$
X = 
\begin{bmatrix}
x_1^0 & \cdots & x_1^p \\
\vdots & \ddots & \vdots \\
x_n^0 & \cdots & x_n^p
\end{bmatrix} 
\in \mathbb{R}^{n \times (p+1)}
$$

and expand the calculations:

$$
\|X \underline{c} - \underline{y}\|^2 = 
\left\|
\begin{bmatrix}
x_1^0 & x_1^1 & \cdots & x_1^p \\
\vdots & \vdots & \ddots & \vdots \\
x_i^0 & x_i^1 & \cdots & x_i^p \\
\vdots & \vdots & \ddots & \vdots \\
x_n^0 & x_n^1 & \cdots & x_n^p
\end{bmatrix}
\begin{bmatrix}
c_0 \\
c_1 \\
\vdots \\
c_p
\end{bmatrix}
-
\begin{bmatrix}
y_1 \\
\vdots \\
y_i \\
\vdots \\
y_n
\end{bmatrix}
\right\|^2
$$

Now, let’s focus on the $$i$$-th element of the norm. After multiplying the $$i$$-th row of $$X$$ with $$\underline{c} $$ and subtracting $$y_i$$, we get:

$$
\left( 
\begin{bmatrix}
x_i^0 & x_i^1 & \cdots & x_i^p
\end{bmatrix}
\begin{bmatrix}
c_0 \\
c_1 \\
\vdots \\
c_p
\end{bmatrix}
- y_i 
\right)^2
= 
\left( \sum_{j=0}^{p} c_j x_i^j - y_i \right)^2
$$

Thus, the $$i$$-th term inside the squared norm is:

$$
\|X \underline{c} - \underline{y}\|^2 = \sum_{i=1}^{n} \left( y_i - \sum_{j=0}^{p} c_j x_i^j \right)^2
$$

which is exactly equation [$$ (3) $$](#eq-matrix). Voilà!

So, we need to minimize this quantity:

$$
\min_{\underline{c}} \|X \underline{c} - \underline{y}\|^2
$$

To solve this problem, we must first understand the mathematical structure of the function to be minimized.

The function $$ \|X \underline{c} - \underline{y}\|^2 $$ is a squared norm, which can be seen as a generalization of the quadratic form $$ ax^2 + bx + c $$, but in higher dimensions.

Graphically, it represents a generalized parabola in multidimensional space, known as a **paraboloid**.

![Light mode only](/paraboloid_light.png){: .light }
![Dark mode only](/paraboloid_dark.png){: .dark }
_A paraboloid. Fascinating, isn’t it?_

This structure is particularly useful because it has one key property: there is a unique global minimum. In one dimension, to find the minimum of a quadratic function, we set the first derivative to zero.

Similarly, in multiple dimensions, we use the **gradient**: we set the gradient of the function to zero to find the point where the minimum is reached. Let’s do this together:

$$
\nabla_{\underline{c}} \left( \underline{c}'X'X\underline{c} - 2\underline{c}'X'\underline{y} + \underline{y}'\underline{y} \right) = \underline{0}
$$

$$
\cancel{2}X'X\underline{c} - \cancel{2}X'\underline{y} = \underline{0}
$$

$$
X'X\underline{c} = X'\underline{y}
$$

Now, to compute $$c_i$$, we have two options:

1. Treat &nbsp; $$ X'X\underline{c} = X'\underline{y} $$ &nbsp; as a linear system in the form $$ A \underline{x} = \underline{b} $$.
2. Alternatively, solve:

$$
\underline{c} = (X'X)^+ X'\underline{y}
$$

where $$(X'X)^+$$ is the generalized inverse of the matrix, known as the **pseudo-inverse**.

<blockquote class="prompt-info">
  <p><strong>The pseudo-inverse</strong> is a mathematical tool used to solve linear systems when the normal inverse of a matrix cannot be computed.</p>

  <p>When does this happen?</p>

  <ol>
    <li>
      <strong>Non-square matrix.</strong>
      <ul>
        <li>This occurs, for instance, when there are more variables than equations, meaning the data is limited compared to the model’s complexity.</li>
      </ul>
      <p>In such cases, the system has infinite solutions, and the pseudo-inverse selects the simplest solution, i.e., the one with the smallest values.</p>
    </li>
    <li>
      <strong>Square but singular matrix.</strong>
      <ul>
        <li>This happens, for example, when there are more equations than variables, meaning there are more observations than the number of variables or parameters we’re trying to estimate.</li>
      </ul>
      <p>In this case, no solution satisfies all the equations, and the pseudo-inverse finds the best possible solution, i.e., the one that minimizes the error between the predicted and actual values.</p>
    </li>
  </ol>
</blockquote>

#### Case 1: Solving a single linear system
In the first case, we consider the system:

$$ X'X\underline{c} = X'\underline{y} $$

which can be represented as:

$$ A \underline{x} = \underline{b} $$
where:

- $$ X'X $$ represents the coefficient matrix $$ A $$.
- $$ X'y $$ is the vector of known terms $$ \underline{b} $$.
- $$ \underline{c} $$ is the vector of unknowns $$ \underline{x} $$.

Let’s calculate its complexity, starting with a simple linear system, for example:

$$
\begin{aligned}
3x + 4y &= 7 \\
7x + 3y &= 4
\end{aligned}
$$

In matrix form:

$$
\begin{bmatrix}
3 & 4 \\
7 & 3
\end{bmatrix}
\cdot
\begin{bmatrix}
x \\
y
\end{bmatrix}
=
\begin{bmatrix}
7 \\
4
\end{bmatrix}
$$

Where:

$$
A =
\begin{bmatrix}
3 & 4 \\
7 & 3
\end{bmatrix}
\quad
x =
\begin{bmatrix}
x \\
y
\end{bmatrix}
\quad
b =
\begin{bmatrix}
7 \\
4
\end{bmatrix}
$$

To solve the system, we can use Gaussian elimination.

We subtract $$\frac{7}{3}$$ times the first row from the second, and the new second row becomes:

$$
0 + \left(-\frac{7}{3} \cdot 4 + 3\right)y = 4 - \frac{7}{3} \cdot 7
$$

In matrix form, we get:

$$
\begin{bmatrix}
3 & 4 \\
0 & -\frac{1}{3}
\end{bmatrix}
$$

In general, if we imagine $$ A $$ as a larger matrix, what we have just obtained would look something like this:

$$
A =
\begin{bmatrix}
. &   &   &   &   &   \\
0 & . &   &   &   &   \\
. & . & . &   &   &   \\
. & . & . & . &   &   \\
. & . & . & . & . &   \\
. & . & . & . & . & .
\end{bmatrix}
$$

We can zero out all the numbers below the first zero obtained:

$$
A =
\begin{bmatrix}
. &   &   &   &   &   \\
0 & . &   &   &   &   \\
0 & . & . &   &   &   \\
0 & . & . & . &   &   \\
0 & . & . & . & . &   \\
0 & . & . & . & . & .
\end{bmatrix}
$$

Then, proceed with the second column below the main diagonal:

$$
A =
\begin{bmatrix}
. &   &   &   &   &   \\
0 & . &   &   &   &   \\
0 & 0 & . &   &   &   \\
0 & 0 & . & . &   &   \\
0 & 0 & . & . & . &   \\
0 & 0 & . & . & . & .
\end{bmatrix}
$$

Until we obtain this:

$$
A =
\begin{bmatrix}
. &   &   &   &   &   \\
0 & . &   &   &   &   \\
0 & 0 & . &   &   &   \\
0 & 0 & 0 & . &   &   \\
0 & 0 & 0 & 0 & . &   \\
0 & 0 & 0 & 0 & 0 & .
\end{bmatrix}
$$

From the last row, we can obtain a first-degree equation in the form:

$$
x = \frac{b}{a}
$$

This solution can then be substituted into the second-to-last row, yielding another first-degree equation of the same form, and so on, back to the top.

In this way, we find the solution to the system.

For a matrix $$n \times n$$, the computational complexity is $$O(n^2)$$ because we need to transform half of the matrix into zeros, which is $$ n \times n = n^2 $$ and dividing by $$2$$ gives $$ \frac{n^2}{2} $$ from which we get $$ O(n^2).$$

#### Case 2: Solving Using the Pseudo-Inverse

Now, let’s consider the more complex case where we solve the system using the pseudo-inverse:

$$
\underline{c} = (X'X)^+ X'\underline{y}
$$

Here, we are looking for an optimal solution that is valid for **all possible vectors** $$\underline{y}$$. In other words, we are analyzing the entire solution space for the matrix $$A = X'X$$ to find the best solution for any possible $$b$$.

For these reasons, calculating the pseudo-inverse has a computational complexity of $$O(n^3)$$, which is higher than that of the first method. This is because it processes more information. Naturally, when performing something more complex, comprehensive, and informative, we need to pay more in terms of memory and computational power.

---

To summarize:
<blockquote class="prompt-tip">
<strong>Case 1: Solving Using the Direct Method</strong>  
<div>
$$
X'X\underline{c} = X'\underline{y}
$$
</div>
Representable as:  
<div>
$$
A \underline{x} = \underline{b}
$$
</div>
<ul>
  <li><strong>Type of Solution</strong>: Finds a specific solution for a single linear system.</li>
  <li><strong>Computational Complexity</strong>: 
  $$ 
  O(n^2) 
  $$  
  because it reduces half of the elements below the main diagonal to zeros.</li>
</ul>

<strong>Case 2: Solving Using the Pseudo-Inverse</strong>  
<div>
$$
\underline{c} = (X'X)^+ X'\underline{y}
$$
</div>
<ul>
  <li><strong>Type of Solution</strong>: Finds an optimal solution valid for all possible vectors \( \underline{y} \) by analyzing the entire solution space.</li>
  <li><strong>Computational Complexity</strong>: 
  $$ 
  O(n^3) 
  $$  
  due to the calculation of the pseudo-inverse, which is more expensive in terms of memory and computational power.</li>
</ul>
</blockquote>

Thus, we have successfully found $$c_i$$. With the data, the prediction function, and the loss function, given $$p$$, we can write the code to compute the optimal coefficients!
